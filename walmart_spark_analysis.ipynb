{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------+-----+-------+\n",
      "|CustomerId|          Name|     City|State|ZipCode|\n",
      "+----------+--------------+---------+-----+-------+\n",
      "|     11039|   Mary Torres|   Caguas|   PR|    725|\n",
      "|      5623|    Jose Haley| Columbus|   OH|  43207|\n",
      "|      5829|    Mary Smith|  Houston|   TX|  77015|\n",
      "|      6336|Richard Maddox|   Caguas|   PR|    725|\n",
      "|      1708|Margaret Booth|Arlington|   TX|  76010|\n",
      "+----------+--------------+---------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------+----------------+---------+--------------------+------+--------+----------+\n",
      "|SalesTxnId|CategoryId|    CategoryName|ProductId|         ProductName| Price|Quantity|CustomerId|\n",
      "+----------+----------+----------------+---------+--------------------+------+--------+----------+\n",
      "|         1|        43|Camping & Hiking|      957|Diamondback Women...|299.98|       1|     11599|\n",
      "|         2|        48|    Water Sports|     1073|Pelican Sunstream...|199.99|       1|       256|\n",
      "|         3|        24| Women's Apparel|      502|Nike Men's Dri-FI...|  50.0|       5|       256|\n",
      "|         4|        18|  Men's Footwear|      403|Nike Men's CJ Eli...|129.99|       1|       256|\n",
      "|         5|        40|     Accessories|      897|Team Golf New Eng...| 24.99|       2|      8827|\n",
      "+----------+----------+----------------+---------+--------------------+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , DoubleType\n",
    "  \n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"walmart sales\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "customer_schema = StructType([ \n",
    "    \n",
    "    StructField('CustomerId', \n",
    "                IntegerType(), True), \n",
    "    StructField('Name', \n",
    "                StringType(), True), \n",
    "    StructField('City', \n",
    "                StringType(), True), \n",
    "    StructField('State', \n",
    "                StringType(), True), \n",
    "    StructField('ZipCode', \n",
    "                IntegerType(), True) \n",
    "]) \n",
    "sales_schema = StructType([ \n",
    "    \n",
    "    StructField('SalesTxnId', \n",
    "                IntegerType(), True), \n",
    "    StructField('CategoryId', \n",
    "                IntegerType(), True),  \n",
    "    StructField('CategoryName', \n",
    "                StringType(), True),\n",
    "    StructField('ProductId', \n",
    "                IntegerType(), True), \n",
    "    StructField('ProductName', \n",
    "                StringType(), True), \n",
    "    StructField('Price', \n",
    "                DoubleType(), True), \n",
    "    StructField('Quantity', \n",
    "                IntegerType(), True),\n",
    "    StructField('CustomerId', \n",
    "                IntegerType(), True) \n",
    "]) \n",
    "#Read tsv file \n",
    "customer_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .schema(customer_schema)\\\n",
    "        .options(header = \"False\", sep = \"\\t\")\\\n",
    "            .load(r\"your file location path\")\n",
    "sales_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .schema(sales_schema)\\\n",
    "        .options(header = \"False\", sep = \"\\t\" )\\\n",
    "            .load(r\"your file location path\")\n",
    "\n",
    "customer_df.show(5)\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Total Number of Customers:\n",
    "How many unique customers are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|TotalUniqueCustomers|\n",
      "+--------------------+\n",
      "|               12346|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "spark.sql(\"\"\"select count(customerid) as TotalUniqueCustomers from (select distinct customerid\n",
    "from customers\n",
    "union \n",
    "select distinct customerid\n",
    "from sales) \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Total Sales by State:\n",
    "What is the total sales amount for each state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|TotalSales|\n",
      "+-----+----------+\n",
      "|   AR|   2136.67|\n",
      "|   AZ|  48702.68|\n",
      "|   CA| 503205.49|\n",
      "|   CO|  40321.13|\n",
      "|   CT|  19206.77|\n",
      "|   DC|   8798.76|\n",
      "|   DE|   1305.76|\n",
      "|   FL|  93359.15|\n",
      "|   GA|  38056.33|\n",
      "|   HI|  35682.81|\n",
      "|   ID|  10098.95|\n",
      "|   IL| 116223.17|\n",
      "|   IN|   6963.97|\n",
      "|   KS|   2999.71|\n",
      "|   KY|   2749.70|\n",
      "|   LA|  24449.42|\n",
      "|   MA|  29039.35|\n",
      "|   MD|  51982.49|\n",
      "|   MI|  83347.09|\n",
      "|   MN|   3549.60|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"salse\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "     c.state,cast(sum(sales.AggrigatedSale) as decimal(10,2)) as TotalSales     \n",
    "from \n",
    "customers c\n",
    " join\n",
    "(select \n",
    "   cast(sum(s.price * s.quantity) as decimal(10,2)) as AggrigatedSale,\n",
    "   s.customerid\n",
    "from \n",
    "salse s\n",
    "group by s.customerid \n",
    ") as sales on c.customerid = sales.customerid group by c.state\n",
    "          order by c.state\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Top 10 Most Purchased Products:\n",
    "Which are the top 10 most purchased products based on the quantity sold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|     Top_10_Products|Quantity|\n",
      "+--------------------+--------+\n",
      "|Perfect Fitness P...|   73698|\n",
      "|Nike Men's Dri-FI...|   62956|\n",
      "|O'Brien Men's Neo...|   57803|\n",
      "|Nike Men's Free 5...|   36680|\n",
      "|Under Armour Girl...|   31735|\n",
      "|Nike Men's CJ Eli...|   22246|\n",
      "|Field & Stream Sp...|   17325|\n",
      "|Pelican Sunstream...|   15500|\n",
      "|Diamondback Women...|   13729|\n",
      "|ENO Atlas Hammock...|     998|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"salse\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "ProductName as Top_10_Products,\n",
    "sum(quantity) as Quantity           \n",
    "from \n",
    "salse\n",
    "group by ProductName\n",
    "ORDER BY Quantity desc limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Average Transaction Value:\n",
    "What is the average price of transactions across all sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|AverageTransactionPrice|\n",
      "+-----------------------+\n",
      "|                 199.32|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select  cast(avg(Totalprice) as decimal(10,2)) as AverageTransactionPrice  from (\n",
    "          select \n",
    "SalesTxnId , cast(sum(price * quantity) as decimal(10,2))  as TotalPrice \n",
    "from     \n",
    "sales \n",
    "group by SalesTxnId) as TotalTransation \n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Top 5 Customers by Expenditure:\n",
    "Who are the top 5 customers by total amount spent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------+\n",
      "|customerId|             Name|TotalSpent|\n",
      "+----------+-----------------+----------+\n",
      "|      9371|   Mary Patterson|   9299.03|\n",
      "|       664|    Bobby Jimenez|   8394.26|\n",
      "|     12431|        Mary Rios|   8073.15|\n",
      "|     10591| Deborah Humphrey|   7889.05|\n",
      "|      9271|Christopher Smith|   7665.25|\n",
      "+----------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "        s.customerId,  c.Name , sum(cast(s.price * s.Quantity as decimal(10,2)) ) as TotalSpent \n",
    "from \n",
    "customers c\n",
    "join \n",
    "sales s\n",
    "on\n",
    "c.customerid = s.customerid\n",
    "group by s.customerid,c.name\n",
    "order by TotalSpent desc limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Product Purchases by a Specific Customer:\n",
    "List all products purchased by a specific customer (e.g., customer with ID 256), including the product name, quantity, and total amount spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+----------+\n",
      "|customerid|         productname|TotalQuantity|TotalSpent|\n",
      "+----------+--------------------+-------------+----------+\n",
      "|       664|Nike Women's Temp...|            5|    150.00|\n",
      "|       664|O'Brien Men's Neo...|            4|    199.92|\n",
      "|       664|Field & Stream Sp...|            8|   3199.84|\n",
      "|       664|Columbia Men's PF...|            3|     90.00|\n",
      "|       664|Glove It Women's ...|            5|     99.95|\n",
      "|       664|Diamondback Women...|            4|   1199.92|\n",
      "|       664|Team Golf San Fra...|            5|    124.95|\n",
      "|       664|Nike Men's Dri-FI...|           10|    500.00|\n",
      "|       664|Pelican Sunstream...|            3|    599.97|\n",
      "|       664|Perfect Fitness P...|           22|   1319.78|\n",
      "|       664|Nike Men's CJ Eli...|            7|    909.93|\n",
      "+----------+--------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "select \n",
    "s.customerid,s.productname,sum(s.quantity) as TotalQuantity,sum(cast(s.price * s.Quantity as decimal(10,2)) ) as TotalSpent  \n",
    "from\n",
    "customers c\n",
    "join \n",
    "sales s\n",
    "on c.customerid = s.customerid\n",
    "where c.customerid = 664\n",
    "group by s.customerid , c.name , s.productname \n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Monthly Sales Trends:\n",
    "Assuming there is a date field, analyze the sales trends over the months. Which month had the highest sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "spark.sql(\"\"\"\n",
    "select  Month , Totalsales  from \n",
    "(select  cast(sum(price * quantity)as decimal(10,2)) as Totalsales, month(date) as Month from\n",
    "sales groupby Month) as TotalMonthSale \n",
    "order by\n",
    "Totalsales desc limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Category with Highest Sales:\n",
    "Which product category generated the highest total sales revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|CategoryName|TotalSales|\n",
      "+------------+----------+\n",
      "|     Fishing|6929653.50|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "spark.sql(\"\"\"\n",
    "select CategoryName , cast(sum(price * quantity) as decimal(10,2)) as TotalSales\n",
    "from \n",
    "sales\n",
    "group by CategoryName\n",
    "order by TotalSales desc limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.State-wise Sales Comparison:\n",
    "Compare the total sales between two specific states (e.g., Texas vs. Ohio). Which state had higher sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+\n",
      "|State|TotalSales|HighestSales|\n",
      "+-----+----------+------------+\n",
      "|   DC|   8798.76|           1|\n",
      "|   MN|   3549.60|           2|\n",
      "+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"\"\"\n",
    "          select *, rank() over(order by TotalSales desc) as HighestSales from \n",
    "          (select * from \n",
    "(select  c.State  , sum(cast(s.Price * s.Quantity as decimal(10,2))) as TotalSales from  sales s join customers c on s.CustomerId = c.CustomerId  \n",
    "          group by c.state ) as statesale where State in ('DC','MN')) as higherSales \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Detailed Customer Purchase Report:\n",
    "Generate a detailed report showing each customer along with their total purchases, the total number of transactions they have made, and the average transaction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+------------------+\n",
      "|CustomerId|TotalTransation|TotalPurchases|AverageTransaction|\n",
      "+----------+---------------+--------------+------------------+\n",
      "|     10054|              1|         59.99|             59.99|\n",
      "|      8985|              1|        399.98|            399.98|\n",
      "|      7909|              1|         99.99|             99.99|\n",
      "|      7091|              1|         49.95|             49.95|\n",
      "|      6372|              1|         74.97|             74.97|\n",
      "|      4158|              1|        199.99|            199.99|\n",
      "|      3278|              1|        129.99|            129.99|\n",
      "|      1476|              1|        129.99|            129.99|\n",
      "|     11347|              2|        189.98|             94.99|\n",
      "|     10465|              2|        249.98|            124.99|\n",
      "|     10243|              2|        329.98|            164.99|\n",
      "|      8798|              2|        599.97|            299.99|\n",
      "|      8783|              2|        519.95|            259.98|\n",
      "|      8659|              2|        204.96|            102.48|\n",
      "|      7308|              2|        499.98|            249.99|\n",
      "|      6226|              2|        699.96|            349.98|\n",
      "|      4843|              2|        549.92|            274.96|\n",
      "|      4777|              2|        549.95|            274.98|\n",
      "|      4638|              2|        289.89|            144.95|\n",
      "|      4546|              2|        339.97|            169.99|\n",
      "+----------+---------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"\"\"\n",
    "select * , cast((TotalPurchases / TotalTransation) as decimal(10,2))  as AverageTransaction   from (\n",
    "select  c.CustomerId ,count(s.SalesTxnId) as TotalTransation , sum(cast(s.Price * s.Quantity as decimal(10,2))) as TotalPurchases \n",
    "from  sales s \n",
    "join customers c on s.CustomerId = c.CustomerId  \n",
    "group by c.CustomerId ) as TransationTable  group by CustomerId,TotalTransation,TotalPurchases order by TotalTransation,CustomerId desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
